{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brazilian-judge",
   "metadata": {},
   "source": [
    "# Your name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-consciousness",
   "metadata": {},
   "source": [
    "This notebook implements timeloop/accelergy-based energy estimation for the neural network model you trained. This part has to be run with the docker we provide, and does not require GPU support. \n",
    "\n",
    "One strategy to reduce the profiling time is to design a model with repeated layers since layers with the same architecture only need one time of profiling.\n",
    "The profiler will also automatically save the information of profiled layers to a .json file specifiled by `profiled_lib_dir`, so that next time the same layer is profiled, the results can be obtained immediately. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-thought",
   "metadata": {},
   "source": [
    "### 1. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "spectacular-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Change this model class to the architecture you used\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "#net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71778055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
    "\n",
    "'''ResNet in PyTorch.\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    " #       self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "class ResNetEff(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNetEff, self).__init__()\n",
    "        self.in_planes = 40\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.layer1 = self._make_layer(block, int(self.in_planes/4), num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, int(self.in_planes/4), num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, int(self.in_planes/4), num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, int(self.in_planes/4), num_blocks[3], stride=2)\n",
    "        self.layer5 = self._make_layer(block, int(self.in_planes/4), num_blocks[3], stride=2)        \n",
    "        self.layer6 = self._make_layer(block, int(self.in_planes/4), num_blocks[4], stride=1)\n",
    "        self.layer7 = self._make_layer(block, int(self.in_planes/2), num_blocks[5], stride=2)\n",
    "        self.layer8 = self._make_layer(block, int(self.in_planes/2), num_blocks[6], stride=2)\n",
    "        self.layer9 = self._make_layer(block, int(self.in_planes/2), num_blocks[7], stride=2)\n",
    "#        self.layer10 = self._make_layer(block, int(self.in_planes/2), num_blocks[8], stride=1)\n",
    "        self.linear = nn.Linear(320, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)      \n",
    "        print(out.shape)             \n",
    "        out = self.layer2(out)\n",
    "        print(out.shape)             \n",
    "        out = self.layer3(out)\n",
    "        print(out.shape)             \n",
    "        out = self.layer4(out) \n",
    "        print(out.shape)         \n",
    "        out = self.layer5(out)         \n",
    "        print(out.shape) \n",
    "        out = self.layer6(out)\n",
    "        print(out.shape)          \n",
    "        out = self.layer7(out)\n",
    "        print(out.shape)          \n",
    "        out = self.layer8(out) \n",
    "        print(out.shape)\n",
    "        out = self.layer9(out)\n",
    "        print(out.shape)\n",
    "#        out = self.layer10(out)\n",
    "        print(out.shape)        \n",
    "        out = F.avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        #print(out.shape)\n",
    "        out = self.linear(out)\n",
    "        return out    \n",
    "\n",
    "def ResNetMake():\n",
    "    return ResNetEff(Bottleneck, [2, 2, 2, 2, 2, 2, 2, 2, 1])    \n",
    "    \n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n",
    "    \n",
    "    \n",
    "net = ResNetMake()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "#optimizer = optim.Adam([var1, var2], lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-hollow",
   "metadata": {},
   "source": [
    "### 2. Run the Profiler to estimate the peak activation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "former-carry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40, 32, 32])\n",
      "torch.Size([1, 40, 16, 16])\n",
      "torch.Size([1, 40, 8, 8])\n",
      "torch.Size([1, 40, 4, 4])\n",
      "torch.Size([1, 40, 2, 2])\n",
      "torch.Size([1, 40, 2, 2])\n",
      "torch.Size([1, 80, 1, 1])\n",
      "torch.Size([1, 160, 1, 1])\n",
      "torch.Size([1, 320, 1, 1])\n",
      "torch.Size([1, 320, 1, 1])\n",
      "torch.Size([1, 40, 32, 32])\n",
      "torch.Size([1, 40, 16, 16])\n",
      "torch.Size([1, 40, 8, 8])\n",
      "torch.Size([1, 40, 4, 4])\n",
      "torch.Size([1, 40, 2, 2])\n",
      "torch.Size([1, 40, 2, 2])\n",
      "torch.Size([1, 80, 1, 1])\n",
      "torch.Size([1, 160, 1, 1])\n",
      "torch.Size([1, 320, 1, 1])\n",
      "torch.Size([1, 320, 1, 1])\n",
      "Peak Activation Sizes: 204800.0 Byte\n"
     ]
    }
   ],
   "source": [
    "from profiler import count_activation_size\n",
    "peak_activation_size = count_activation_size(\n",
    "    net=net,\n",
    "    input_size=(1, 3, 32, 32),\n",
    ")\n",
    "\n",
    "print(f\"Peak Activation Sizes: {peak_activation_size} Byte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-advertising",
   "metadata": {},
   "source": [
    "### 3. Run the Profiler for Timeloop/Accelergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "needed-tribune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting nn.Conv2d and nn.Linear in network-Feb-28-2022 model ...\n",
      "torch.Size([2, 40, 32, 32])\n",
      "torch.Size([2, 40, 16, 16])\n",
      "torch.Size([2, 40, 8, 8])\n",
      "torch.Size([2, 40, 4, 4])\n",
      "torch.Size([2, 40, 2, 2])\n",
      "torch.Size([2, 40, 2, 2])\n",
      "torch.Size([2, 80, 1, 1])\n",
      "torch.Size([2, 160, 1, 1])\n",
      "torch.Size([2, 320, 1, 1])\n",
      "torch.Size([2, 320, 1, 1])\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer1.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer2.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer3.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer4.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer5.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer6.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer7.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer8.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer9.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer10.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer11.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer12.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer13.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer14.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer15.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer16.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer17.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer18.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer19.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer20.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer21.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer22.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer23.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer24.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer25.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer26.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer27.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer28.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer29.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer30.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer31.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer32.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer33.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer34.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer35.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer36.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer37.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer38.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer39.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer40.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer41.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer42.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer43.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer44.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer45.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer46.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer47.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer48.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer49.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer50.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer51.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer52.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer53.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer54.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer55.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer56.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer57.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer58.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer59.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer60.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer61.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer62.yaml\n",
      "workload file --> /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer63.yaml\n",
      "conversion complete!\n",
      "\n",
      "running timeloop to get energy and latency...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [03:55<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeloop running finished!\n",
      "torch.Size([1, 40, 32, 32])\n",
      "torch.Size([1, 40, 16, 16])\n",
      "torch.Size([1, 40, 8, 8])\n",
      "torch.Size([1, 40, 4, 4])\n",
      "torch.Size([1, 40, 2, 2])\n",
      "torch.Size([1, 40, 2, 2])\n",
      "torch.Size([1, 80, 1, 1])\n",
      "torch.Size([1, 160, 1, 1])\n",
      "torch.Size([1, 320, 1, 1])\n",
      "torch.Size([1, 320, 1, 1])\n",
      "torch.Size([1, 40, 32, 32])\n",
      "torch.Size([1, 40, 16, 16])\n",
      "torch.Size([1, 40, 8, 8])\n",
      "torch.Size([1, 40, 4, 4])\n",
      "torch.Size([1, 40, 2, 2])\n",
      "torch.Size([1, 40, 2, 2])\n",
      "torch.Size([1, 80, 1, 1])\n",
      "torch.Size([1, 160, 1, 1])\n",
      "torch.Size([1, 320, 1, 1])\n",
      "torch.Size([1, 320, 1, 1])\n",
      "torch.Size([1, 40, 32, 32])\n",
      "torch.Size([1, 40, 16, 16])\n",
      "torch.Size([1, 40, 8, 8])\n",
      "torch.Size([1, 40, 4, 4])\n",
      "torch.Size([1, 40, 2, 2])\n",
      "torch.Size([1, 40, 2, 2])\n",
      "torch.Size([1, 80, 1, 1])\n",
      "torch.Size([1, 160, 1, 1])\n",
      "torch.Size([1, 320, 1, 1])\n",
      "torch.Size([1, 320, 1, 1])\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer1 \t Energy: 30511892.24 \t Cycle: 36864 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer2 \t Energy: 11336153.94 \t Cycle: 4096 \t Number of same architecture layers: 3\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer3 \t Energy: 23989316.92 \t Cycle: 9216 \t Number of same architecture layers: 2\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer4 \t Energy: 10456251.98 \t Cycle: 4096 \t Number of same architecture layers: 2\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer9 \t Energy: 6086527.79 \t Cycle: 2304 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer10 \t Energy: 2653706.80 \t Cycle: 1024 \t Number of same architecture layers: 2\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer11 \t Energy: 11598957.63 \t Cycle: 4096 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer12 \t Energy: 2873682.29 \t Cycle: 1024 \t Number of same architecture layers: 2\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer13 \t Energy: 6086527.79 \t Cycle: 2304 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer16 \t Energy: 1284887.53 \t Cycle: 576 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer17 \t Energy: 703070.50 \t Cycle: 256 \t Number of same architecture layers: 2\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer18 \t Energy: 3060935.02 \t Cycle: 1024 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer19 \t Energy: 758064.38 \t Cycle: 256 \t Number of same architecture layers: 2\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer20 \t Energy: 1007134.93 \t Cycle: 576 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer23 \t Energy: 421757.64 \t Cycle: 144 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer24 \t Energy: 215411.43 \t Cycle: 64 \t Number of same architecture layers: 2\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer25 \t Energy: 1576085.86 \t Cycle: 320 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer26 \t Energy: 229159.90 \t Cycle: 64 \t Number of same architecture layers: 2\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer27 \t Energy: 355402.37 \t Cycle: 144 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer30 \t Energy: 200103.02 \t Cycle: 36 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer31 \t Energy: 91681.37 \t Cycle: 16 \t Number of same architecture layers: 4\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer32 \t Energy: 387735.11 \t Cycle: 64 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer33 \t Energy: 92851.34 \t Cycle: 16 \t Number of same architecture layers: 3\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer34 \t Energy: 186597.08 \t Cycle: 36 \t Number of same architecture layers: 3\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer42 \t Energy: 193867.56 \t Cycle: 32 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer43 \t Energy: 536854.05 \t Cycle: 36 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer44 \t Energy: 237783.64 \t Cycle: 10 \t Number of same architecture layers: 2\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer45 \t Energy: 465327.28 \t Cycle: 20 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer46 \t Energy: 239761.27 \t Cycle: 10 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer47 \t Energy: 536854.05 \t Cycle: 36 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer49 \t Energy: 468502.57 \t Cycle: 20 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer50 \t Energy: 2089446.89 \t Cycle: 144 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer51 \t Energy: 924782.41 \t Cycle: 40 \t Number of same architecture layers: 2\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer52 \t Energy: 1829084.82 \t Cycle: 50 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer53 \t Energy: 931487.50 \t Cycle: 40 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer54 \t Energy: 2089446.89 \t Cycle: 144 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer56 \t Energy: 1830589.11 \t Cycle: 50 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer57 \t Energy: 8201811.01 \t Cycle: 225 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer58 \t Energy: 3646425.34 \t Cycle: 100 \t Number of same architecture layers: 2\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer59 \t Energy: 7251890.68 \t Cycle: 200 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer60 \t Energy: 3650938.23 \t Cycle: 100 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer61 \t Energy: 8201811.01 \t Cycle: 225 \t Number of same architecture layers: 1\n",
      "Name: /home/workspace/lab2/workloads/network-Feb-28-2022/network-Feb-28-2022_layer63 \t Energy: 488003.92 \t Cycle: 20 \t Number of same architecture layers: 1\n",
      "\n",
      "Total Energy: 0.23017247 mj \n",
      "Total Cycles: 0.09461200 Million\n",
      "MACs: 7282260\n",
      "Num of Parameters: 373410 \n",
      "Peak Activation Size: 204800.0 Byte\n"
     ]
    }
   ],
   "source": [
    "from profiler import Profiler\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "sub_dir = \"network-\" + today.strftime(\"%b-%d-%Y\")\n",
    "\n",
    "profiler = Profiler(\n",
    "    top_dir='workloads',\n",
    "    sub_dir=sub_dir,\n",
    "    timeloop_dir='simple_weight_stationary',\n",
    "    model=net,\n",
    "    input_size=(3, 32, 32),\n",
    "    batch_size=1,\n",
    "    convert_fc=True,\n",
    "    exception_module_names=[],\n",
    "    profiled_lib_dir=f\"profiled_lib.json\"\n",
    ")\n",
    "\n",
    "layer_wise, overall = profiler.profile()\n",
    "\n",
    "for layer_id, info in layer_wise.items():\n",
    "    print(f\"Name: {info['name']} \\t Energy: {info['energy']:.2f} \\t Cycle: {info['cycle']} \\t Number of same architecture layers: {info['num']}\")\n",
    "    \n",
    "print(f\"\\nTotal Energy: {overall['total_energy']/1e9:.8f} mj \\nTotal Cycles: {overall['total_cycle']/1e6:.8f} Million\")\n",
    "\n",
    "print(f\"MACs: {overall['macs']}\\nNum of Parameters: {overall['num_params']} \\nPeak Activation Size: {overall['activation_size']} Byte\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-drill",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
