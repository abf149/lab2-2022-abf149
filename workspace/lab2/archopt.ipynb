{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Code\n",
    "\n",
    "This notebook implements starter code to help you get started with the assignment. If you have a personal GPU, copy this starter code to another directory and run this notebook using your personal GPU. In that case, the docker we provide will not support PyTorch with GPU, and you can create your own virtual environment (e.g., conda) with PyTorch support (https://pytorch.org/get-started/locally/). \n",
    "\n",
    "To run this notebook you must __first create a folder called `./dataset` and download all the data files from the Kaggle competition page__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "%matplotlib inline\n",
    "\n",
    "log_interval = 1000,\n",
    "epochs = 1\n",
    "\n",
    "params = {\n",
    "    'log_interval': 1000,\n",
    "    'epochs': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "# GPU setup\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Using {}\".format('GPU' if use_cuda else 'CPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract a \"one-of-N\" or N-way one-hot decision (usually for assigning a factor of a rank to a memory level)\n",
    "#\n",
    "# Initialization:\n",
    "# - num_factors: number of decision ways (factors) to choose between\n",
    "#\n",
    "# Input: \n",
    "# - x: [factors, baselines, temperature]\n",
    "# - factors is a list length num_factors containing the factors to select among\n",
    "# - baselines is a list of length num_factors containing the way-wise value that \n",
    "#   an output way should have when it is unselected\n",
    "# - temperature is a software parameter; high temperature smooths the softmax, \n",
    "#   low temperature approximates a discrete decision\n",
    "#\n",
    "# Trainable parameters:\n",
    "# - W: way-wise weights input to softmax\n",
    "#\n",
    "# Output: tensor of num_factors outputs. The way i which is weighted highest in W \n",
    "#         should be railed to factors[i], the other ways should be weighted to baselines[i]\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class FactorMux(nn.Module):\n",
    "    def __init__(self, num_factors):\n",
    "        super(FactorMux, self).__init__()\n",
    "        self.num_factors = num_factors\n",
    "        self.W = torch.nn.Parameter(torch.randn(self.num_factors))\n",
    "        self.W.requires_grad = True\n",
    "        self.softmax1 = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [factors, baselines, temperature]\n",
    "        \n",
    "        assert len(x) == 2*self.num_factors + 1\n",
    "        \n",
    "        factors = x[0:self.num_factors]\n",
    "        baselines = x[self.num_factors:(2*self.num_factors)]\n",
    "        temperature = x[2*self.num_factors].item()\n",
    "        \n",
    "        #print(factors, baselines, 1.0/temperature)\n",
    "        \n",
    "        x = self.softmax1((1.0/temperature)*self.W) * (factors - baselines) + baselines\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "fmux = FactorMux(2)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 1.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test FactorMux\n",
    "fmux.W.data = torch.tensor([1, 0])\n",
    "fmux(torch.tensor([2.0, 2.0, 1.0, 1.0, 0.01]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LoopNestSelector(FactorMux):\n",
    "    def __init__(self, num_datatypes):\n",
    "        super(LoopNestSelector, self).__init__(num_datatypes) # FactorMux num_factors\n",
    "        self.baseline = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [factors, temperature]\n",
    "\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.tensor(x)\n",
    "        \n",
    "        reuse_factors = x[0:self.num_factors]\n",
    "        temperature = torch.tensor([x[self.num_factors].item()])\n",
    "        \n",
    "        #print(reuse_factors, reuse_factors*0.0 + self.baseline, temperature)\n",
    "        \n",
    "        inputs = torch.cat((reuse_factors, reuse_factors*0.0 + self.baseline, temperature), 0)\n",
    "        x = super().forward(inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 4., 1.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmux = LoopNestSelector(3)\n",
    "rmux.W.data = torch.tensor([0.0,0.1,0.0])\n",
    "rmux([3, 4, 5, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RankLoopBoundSelector(nn.Module):\n",
    "    def __init__(self, num_rank_loops, rank_factor_list):\n",
    "        super(RankLoopBoundSelector, self).__init__() # FactorMux\n",
    "            \n",
    "        if not torch.is_tensor(rank_factor_list):\n",
    "            rank_factor_list = torch.tensor(rank_factor_list)            \n",
    "        \n",
    "        self.num_rank_loops = num_rank_loops\n",
    "        self.rank_factor_list = rank_factor_list\n",
    "        self.num_rank_factors = len(self.rank_factor_list)\n",
    "        self.factorMuxes = nn.ModuleList([FactorMux(self.num_rank_loops) for factor in self.rank_factor_list])\n",
    "        \n",
    "        self.baseline = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [temperature]\n",
    "\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.tensor([x])\n",
    "        \n",
    "        x = torch.stack( \\\n",
    "            [self.factorMuxes[mdx](\n",
    "            torch.cat( \\\n",
    "            (torch.full((self.num_rank_loops,), self.rank_factor_list[mdx]), torch.full((self.num_rank_loops,), self.baseline), \\\n",
    "            x),0) \\\n",
    "            ) \\\n",
    "            for mdx in range(self.num_rank_factors)])\n",
    "        \n",
    "        x = torch.prod(x,0)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        #x = torch.tensor([  for vec in x])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlbs = RankLoopBoundSelector(3, [3, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 5.], grad_fn=<ProdBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "print(rlbs(0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([ 0.0168,  0.2230, -0.1170], requires_grad=True), Parameter containing:\n",
       " tensor([ 1.5208, -0.1559, -1.6177], requires_grad=True), Parameter containing:\n",
       " tensor([-0.5039, -0.8544,  0.7811], requires_grad=True)]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rlbs.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 61]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "\n",
    "primes(244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.426264754702098"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(86,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MapSelector_spGEMM(nn.Module):\n",
    "    \n",
    "    def repeat_prime(self, n, d):\n",
    "        lst = [d]\n",
    "        fac = d\n",
    "\n",
    "        while n % (fac * d) == 0:\n",
    "            fac = fac * d\n",
    "            lst.append(d)\n",
    "\n",
    "        return lst\n",
    "\n",
    "    # https://stackoverflow.com/questions/16996217/prime-factorization-list\n",
    "    def primes(self, n):\n",
    "        divisors = [ self.repeat_prime(n, d) for d in range(2,n//2+1) if n % d == 0 ]\n",
    "        divisors = sum(divisors, [])\n",
    "        return [ d for d in divisors if \\\n",
    "                 all( d % od != 0 for od in divisors if od != d ) ]    \n",
    "    \n",
    "    def __init__(self, M, K, N):\n",
    "        super(MapSelector_spGEMM, self).__init__() # FactorMux\n",
    "        \n",
    "        self.M = M\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.M_factors = self.primes(self.M)\n",
    "        self.K_factors = self.primes(self.K)\n",
    "        self.N_factors = self.primes(self.N)\n",
    "        self.main_mem_loops = 2\n",
    "        self.local_mem_loops = 1\n",
    "        self.total_loops = self.main_mem_loops + self.local_mem_loops\n",
    "        \n",
    "        # Loop bounds\n",
    "        self.M_bound_selector = RankLoopBoundSelector(self.total_loops, self.M_factors)\n",
    "        self.K_bound_selector = RankLoopBoundSelector(self.total_loops, self.K_factors)\n",
    "        self.N_bound_selector = RankLoopBoundSelector(self.total_loops, self.N_factors)        \n",
    "        \n",
    "        # Loop nest ordering\n",
    "        self.num_datatypes = 3\n",
    "        self.main_mem_temporal_LoopNestSelector = LoopNestSelector(self.num_datatypes)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [temperature]\n",
    "\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.tensor([x])\n",
    "        \n",
    "        # Calculate loop bounds\n",
    "        M_loop_bounds = self.M_bound_selector(x)\n",
    "        K_loop_bounds = self.K_bound_selector(x)\n",
    "        N_loop_bounds = self.N_bound_selector(x)        \n",
    "        M_main_mem_temporal = M_loop_bounds[1][None] # B reuse ceiling\n",
    "        K_main_mem_temporal = K_loop_bounds[1][None] # Z reuse ceiling\n",
    "        N_main_mem_temporal = N_loop_bounds[1][None] # A reuse ceiling\n",
    "        \n",
    "        #print(torch.flatten(torch.cat((N_main_mem_temporal, M_main_mem_temporal, K_main_mem_temporal, x),0)))\n",
    "        \n",
    "        # Calculate loop nest ordering\n",
    "        main_mem_loop_nest = self.main_mem_temporal_LoopNestSelector( \\\n",
    "            torch.flatten(torch.cat((N_main_mem_temporal, M_main_mem_temporal, K_main_mem_temporal, x),0)))\n",
    "        \n",
    "        #print(M_loop_bounds,K_loop_bounds,N_loop_bounds, main_mem_loop_nest)        \n",
    "        \n",
    "        x = torch.cat((M_loop_bounds,K_loop_bounds,N_loop_bounds, main_mem_loop_nest),0)\n",
    "        \n",
    "        # torch.tensor([L1 spatial bound & L1 temporal bound & L0 temporal bound For ranks M, K, N; reuse factor for A, B, Z])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  2.,  3.,  5.,  5.,  1.,  1., 21.,  1.,  1.,  2.,  1.],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSs = MapSelector_spGEMM(2*3, 5*5, 3*7)\n",
    "MSs(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ArchMap_spGEMM(nn.Module): \n",
    "    \n",
    "    def __init__(self, M, K, N):\n",
    "        super(ArchMap_spGEMM, self).__init__() # FactorMux\n",
    "        \n",
    "        self.M = M\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.mapper = MapSelector_spGEMM(M, K, N)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [temperature]\n",
    "\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.tensor([x])\n",
    "        \n",
    "        # Problem characteristics\n",
    "        M = float(self.M)\n",
    "        K = float(self.K)\n",
    "        N = float(self.N)\n",
    "        A_matrix_size = M*K\n",
    "        B_matrix_size = K*N\n",
    "        Z_matrix_size = M*N\n",
    "        main_memory_size = A_matrix_size+B_matrix_size+Z_matrix_size\n",
    "        total_num_macs = M*K*N\n",
    "        \n",
    "        # Calculate map (loop bounds & loop nest)\n",
    "        # torch.tensor([L1 spatial bound & L1 temporal bound & L0 temporal bound For ranks M, K, N; reuse factor for A, B, Z])\n",
    "        x = self.mapper(x)\n",
    "        \n",
    "        self.x = x\n",
    "        \n",
    "        # Break out map parameters\n",
    "        M_L1_spatial_bound = x[0]\n",
    "        M_L1_temporal_bound = x[1]\n",
    "        M_L0_temporal_bound = x[2]\n",
    "        K_L1_spatial_bound = x[3]\n",
    "        K_L1_temporal_bound = x[4]\n",
    "        K_L0_temporal_bound = x[5]\n",
    "        N_L1_spatial_bound = x[6]\n",
    "        N_L1_temporal_bound = x[7]\n",
    "        N_L0_temporal_bound = x[8]        \n",
    "        A_reuse = x[9]\n",
    "        B_reuse = x[10]\n",
    "        Z_reuse = x[11]\n",
    "\n",
    "        # Hook\n",
    "        self.M_L1_spatial_bound = M_L1_spatial_bound\n",
    "        self.M_L1_temporal_bound = M_L1_temporal_bound\n",
    "        self.M_L0_temporal_bound = M_L0_temporal_bound\n",
    "        self.K_L1_spatial_bound = K_L1_spatial_bound\n",
    "        self.K_L1_temporal_bound = K_L1_temporal_bound\n",
    "        self.K_L0_temporal_bound = K_L0_temporal_bound\n",
    "        self.N_L1_spatial_bound = N_L1_spatial_bound\n",
    "        self.N_L1_temporal_bound = N_L1_temporal_bound\n",
    "        self.N_L0_temporal_bound = N_L0_temporal_bound       \n",
    "        self.A_reuse = A_reuse\n",
    "        self.B_reuse = B_reuse\n",
    "        self.Z_reuse = Z_reuse \n",
    "        \n",
    "        # Spatial characteristics\n",
    "        spatial_fan_out = M_L1_spatial_bound*K_L1_spatial_bound*N_L1_spatial_bound\n",
    "        \n",
    "        # Tiling\n",
    "        main_memory_tile_pair_count = M*K*N/M_L0_temporal_bound/K_L0_temporal_bound/N_L0_temporal_bound\n",
    "        A_tile_size = M_L0_temporal_bound*K_L0_temporal_bound\n",
    "        B_tile_size = K_L0_temporal_bound*N_L0_temporal_bound\n",
    "        Z_tile_size = M_L0_temporal_bound*N_L0_temporal_bound \n",
    "        local_memory_num_macs_per_tile = M_L0_temporal_bound*K_L0_temporal_bound*N_L0_temporal_bound\n",
    "        \n",
    "        # Per-PE characteristics\n",
    "        local_memory_size = A_tile_size + B_tile_size + Z_tile_size\n",
    "        local_num_macs = M*K*N/spatial_fan_out\n",
    "        \n",
    "        # Global characteristics\n",
    "        total_local_memory_size = local_memory_size*spatial_fan_out\n",
    "        \n",
    "        # Access counts\n",
    "        main_memory_reads = main_memory_tile_pair_count*(A_tile_size/A_reuse + B_tile_size/B_reuse + Z_tile_size/Z_reuse)\n",
    "        main_memory_writes = main_memory_tile_pair_count*(Z_tile_size/Z_reuse)\n",
    "        local_memory_reads = local_num_macs*3 # A, B, Z\n",
    "        local_memory_writes = local_num_macs # Just Z\n",
    "        total_local_memory_reads = local_memory_reads * spatial_fan_out\n",
    "        total_local_memory_writes = local_memory_writes * spatial_fan_out\n",
    "        \n",
    "        # Energy constants (currently arbitrary)\n",
    "        E_DRAM_static = 0.05\n",
    "        E_SRAM_static = 0.02\n",
    "        E_DRAM_acc = 1.0\n",
    "        E_SRAM_acc = 0.7\n",
    "        E_MAC = 0.1\n",
    "        \n",
    "        # Scale energy consumption of memories to memory size\n",
    "        main_memory_unit_read_energy = E_DRAM_acc*main_memory_size\n",
    "        main_memory_unit_write_energy = main_memory_unit_read_energy\n",
    "        local_memory_static_energy = E_SRAM_static*local_memory_size\n",
    "        local_memory_unit_read_energy = E_SRAM_acc*local_memory_size\n",
    "        local_memory_unit_write_energy = local_memory_unit_read_energy\n",
    "        \n",
    "        # Calculate energy consumption\n",
    "        total_main_memory_access_energy = main_memory_reads*main_memory_unit_read_energy + \\\n",
    "                                          main_memory_writes*main_memory_unit_write_energy\n",
    "        total_local_memory_static_energy = local_memory_static_energy*spatial_fan_out\n",
    "        total_local_memory_access_energy = total_local_memory_reads*local_memory_unit_read_energy + \\\n",
    "                                           total_local_memory_writes*local_memory_unit_write_energy\n",
    "        total_MAC_energy = total_num_macs*E_MAC\n",
    "        total_problem_energy = total_MAC_energy + total_local_memory_access_energy + \\\n",
    "                               total_local_memory_static_energy + total_main_memory_access_energy\n",
    "        \n",
    "        # Calculate latency in cycles\n",
    "        total_problem_latency = local_num_macs\n",
    "        \n",
    "        # Calculate EDP\n",
    "        problem_edp = total_problem_energy*total_problem_latency\n",
    "        \n",
    "        return problem_edp\n",
    "    \n",
    "    def calcMap(self,x):\n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "net = ArchMap_spGEMM(2*3, 5*5, 3*7)\n",
    "T = torch.tensor([1.0])\n",
    "net(T)\n",
    "optimizer = optim.Adam(net.parameters() , lr=0.002) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.tensor([1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad() # clear gradients\n",
    "edp = net(T) # forward step\n",
    "loss = edp\n",
    "loss.backward() # backprop\n",
    "optimizer.step() # optimize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.4132e+08, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature:  1.0\n",
      "Map:  tensor([1.1679, 2.5330, 3.2106, 7.4978, 3.4895, 5.2947, 6.2143, 2.9567, 4.3782,\n",
      "        2.1663, 1.3767, 1.3938], grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(1.1521e+08, grad_fn=<MulBackward0>) 100.0\n",
      "tensor(15576613., grad_fn=<MulBackward0>) 49.13805675949616\n",
      "tensor(10575188., grad_fn=<MulBackward0>) 11.804454609030374\n",
      "tensor(9378291., grad_fn=<MulBackward0>) 4.582935465801569\n",
      "tensor(8876133., grad_fn=<MulBackward0>) 2.258720184090848\n",
      "tensor(8621956., grad_fn=<MulBackward0>) 1.225247852267779\n",
      "tensor(8486341., grad_fn=<MulBackward0>) 0.6689809940574168\n",
      "tensor(8413010., grad_fn=<MulBackward0>) 0.369918082039226\n",
      "tensor(8371231., grad_fn=<MulBackward0>) 0.2155650750368384\n",
      "tensor(8346455.5000, grad_fn=<MulBackward0>) 0.12912118976722647\n",
      "tensor(8331535., grad_fn=<MulBackward0>) 0.07818359797104028\n",
      "tensor(8322474., grad_fn=<MulBackward0>) 0.04763136351143312\n",
      "tensor(8316937., grad_fn=<MulBackward0>) 0.029130820341810842\n",
      "tensor(8313554., grad_fn=<MulBackward0>) 0.017859204525774983\n",
      "tensor(8311478.5000, grad_fn=<MulBackward0>) 0.01091142455625119\n",
      "tensor(8310205., grad_fn=<MulBackward0>) 0.006744264006470643\n",
      "tensor(8309423., grad_fn=<MulBackward0>) 0.004127673142199603\n",
      "tensor(8308941.5000, grad_fn=<MulBackward0>) 0.00257547216588544\n",
      "tensor(8308648., grad_fn=<MulBackward0>) 0.0015525751595316108\n",
      "tensor(8308471., grad_fn=<MulBackward0>) 0.0009568456589898358\n",
      "tensor(8308360., grad_fn=<MulBackward0>) 0.0005837460076587477\n",
      "tensor(8308290.5000, grad_fn=<MulBackward0>) 0.00035506567992911204\n",
      "tensor(8308252.5000, grad_fn=<MulBackward0>) 0.00022266967459294478\n",
      "tensor(8308225.5000, grad_fn=<MulBackward0>) 0.00012036248607029904\n",
      "tensor(8308211., grad_fn=<MulBackward0>) 7.823580427881243e-05\n",
      "tensor(8308202., grad_fn=<MulBackward0>) 3.610887837879802e-05\n",
      "tensor(8308194., grad_fn=<MulBackward0>) 5.4163359943390865e-05\n",
      "tensor(8308189.5000, grad_fn=<MulBackward0>) 6.018155088819543e-06\n",
      "Percentage threshold reached\n",
      "Temperature:  0.5\n",
      "Map:  tensor([ 5.7123,  1.0000,  1.1438, 19.0101,  1.7878,  1.6974, 18.9327,  1.0000,\n",
      "         1.6891,  1.0000,  1.0000,  1.7878], grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(8308186.5000, grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  0.25\n",
      "Map:  tensor([ 5.7123,  1.0000,  1.1438, 19.0100,  1.7879,  1.6974, 18.9327,  1.0000,\n",
      "         1.6891,  1.0000,  1.0000,  1.7879], grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(8308187., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  0.125\n",
      "Map:  tensor([ 5.7123,  1.0000,  1.1438, 19.0100,  1.7879,  1.6974, 18.9327,  1.0000,\n",
      "         1.6891,  1.0000,  1.0000,  1.7879], grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(8308186.5000, grad_fn=<MulBackward0>) 6.01816015937051e-06\n",
      "Percentage threshold reached\n",
      "Temperature:  0.0625\n",
      "Map:  tensor([ 5.7123,  1.0000,  1.1438, 19.0100,  1.7879,  1.6974, 18.9327,  1.0000,\n",
      "         1.6891,  1.0000,  1.0000,  1.7879], grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(8308187., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  0.03125\n",
      "Map:  tensor([ 5.7123,  1.0000,  1.1438, 19.0100,  1.7879,  1.6974, 18.9327,  1.0000,\n",
      "         1.6891,  1.0000,  1.0000,  1.7879], grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(8308186., grad_fn=<MulBackward0>) 1.203632031874102e-05\n",
      "tensor(8308186., grad_fn=<MulBackward0>) 4.21271287214278e-05\n",
      "tensor(8308186.5000, grad_fn=<MulBackward0>) 1.2036322491836515e-05\n",
      "tensor(8308186., grad_fn=<MulBackward0>) 6.01816015937051e-06\n",
      "Percentage threshold reached\n",
      "Temperature:  0.015625\n",
      "Map:  tensor([ 5.7123,  1.0000,  1.1438, 19.0100,  1.7879,  1.6974, 18.9326,  1.0000,\n",
      "         1.6891,  1.0000,  1.0000,  1.7879], grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(8308186., grad_fn=<MulBackward0>) 1.203632031874102e-05\n",
      "tensor(8308186., grad_fn=<MulBackward0>) 6.0181612459182576e-06\n",
      "Percentage threshold reached\n",
      "Temperature:  0.0078125\n",
      "Map:  tensor([ 5.7123,  1.0000,  1.1438, 19.0100,  1.7879,  1.6974, 18.9327,  1.0000,\n",
      "         1.6891,  1.0000,  1.0000,  1.7879], grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(8360469., grad_fn=<MulBackward0>) 1.7941573981990606e-05\n",
      "tensor(8360465.5000, grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  0.00390625\n",
      "Map:  tensor([ 6.0000,  1.0000,  1.0000, 19.3368,  1.7312,  1.6561, 19.1039,  1.0000,\n",
      "         1.6320,  1.0000,  1.0000,  1.7312], grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  0.001953125\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  0.0009765625\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  0.00048828125\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  0.000244140625\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  0.0001220703125\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  6.103515625e-05\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  3.0517578125e-05\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  1.52587890625e-05\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  7.62939453125e-06\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  3.814697265625e-06\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  1.9073486328125e-06\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  9.5367431640625e-07\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  4.76837158203125e-07\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  2.384185791015625e-07\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  1.1920928955078125e-07\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  5.960464477539063e-08\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  2.9802322387695312e-08\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  1.4901161193847656e-08\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  7.450580596923828e-09\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  3.725290298461914e-09\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  1.862645149230957e-09\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  9.313225746154785e-10\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  4.656612873077393e-10\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  2.3283064365386963e-10\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  1.1641532182693481e-10\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  5.820766091346741e-11\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  2.9103830456733704e-11\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  1.4551915228366852e-11\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  7.275957614183426e-12\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>) 0.0\n",
      "Percentage threshold reached\n",
      "Temperature:  3.637978807091713e-12\n",
      "Map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n",
      "-----------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-476-7f0b844196ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0medp_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# optimize weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abf149/.local/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abf149/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "net = ArchMap_spGEMM(2*3, 5*5, 3*7)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters() , lr=0.002)\n",
    "\n",
    "T = torch.tensor([1.0])\n",
    "\n",
    "edp_list = []\n",
    "\n",
    "thresh_pct = 0.5\n",
    "\n",
    "pct = 100.0\n",
    "\n",
    "i = 0\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "line1, = ax.plot(np.array(list(range(len(edp_list)))),np.array(edp_list), 'r-') # Returns a tuple of line objects, thus the comma\n",
    "\n",
    "net(T)\n",
    "\n",
    "while(True):\n",
    "    print(\"Temperature: \", T.item())\n",
    "    print(\"Map: \", net.calcMap(T))\n",
    "    print(\"-----------------------\")\n",
    "    pct = 100.0\n",
    "    while(pct > 0.00001):\n",
    "        optimizer.zero_grad() # clear gradients\n",
    "        edp = net(T) # forward step\n",
    "        edp_list.append(edp.item())\n",
    "        loss = edp\n",
    "        loss.backward() # backprop\n",
    "        optimizer.step() # optimize weights\n",
    "        if i % 1000 == 0:\n",
    "            pct = 100.0\n",
    "            if i > 0:\n",
    "                pct = 100.0*abs(edp_list[-1]-edp_list[-500])/edp_list[-500] #100.0*abs(edp_list[-1]-edp_list[-2])/edp_list[-2]\n",
    "            print(net(T), pct)    \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        i = i + 1\n",
    "    print(\"Percentage threshold reached\")\n",
    "    T = T/2.0\n",
    "\n",
    "    \n",
    "# len(edp_list)<2 or 100.0*math.abs(edp_list[-1]-edp_list[-2])/edp_list[-2] > thresh_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZwcVbn/8c8ze8iOCZNAYsImkiACGVkEZVDQgAroxSuoCF4x14XfdeMqKHIRcVfEBQV+LuBFCeBPFDGKiAyI7AESSCAhhCWBhAQISSbrLM/vj3M6qTQ909Uz0zPd1vf9es1rauuqp6qr66lzTi3m7oiIiOSrGeoARESkMilBiIhIQUoQIiJSkBKEiIgUpAQhIiIFKUGIiEhBShCAmT1lZkfH7i+a2c+GMJY2MztjqJZfScxsgZm19vGzfzaz0wY4pH7F1I9lmpn90szWmNm9g7nsUg3F9hksZna+mV3Vy/h/uXWvG+oAijGzk4HPAPsBG4AngSuBn3oZbuJw968PxHzMbCoh1np37xyIeQ6lcq+PmV0BLHf3c3PD3H16X+fn7sdWWkz9cARwDDDJ3TcMwfJTG6LtUxGS625m5wN7ufsH+ztfM3sKOMPd/9bfeZWqoksQZvY54AfAd4AJQDPwMeBwoKGHz9QOWoAig2MK8FSlJ4fBZGYDfnJoZhV/wtwX/Vovd6/IP2A0ocTwb0WmuwL4KTAnTn808A7gQWAdsAw4P+8zpwJPAy8CXwKeAo6O484HrkpMeyhwJ/AyMA9oTYxrA74K/BNYD/wVGBfHPQM40B7/Dush/mOAx4C1wI+B2whnC7nx/wE8CqwBbgKmxOEGfB9YFdfzYWC/OG4Y8L24jmuBO4Bh5V4foBG4GHgu/l0MNMZxrcBy4IvAC3GbfyCOmwV0AFvjvP8Yh+d/L9cBV8XYHgZeA5wTt8Ey4G1563JG7J6XiLs9rkdrHHcdsDJup9uB6SXElGZ9PxfjWwF8uJf9eFfgBuAlYAnw0Tj8I8BmoCvG8ZUCnz09fmffj9/rUuCNcfiyuPzTEtP3+PsApsbtMyuu0wrgrMT484HfAtfE7+EB4PWJ8fnf2bXAr+K0C4CWxLQHxTjWx+/hGuDClMcHTzndwcBdcbusIPzGGpLzAT4JPA48GYdNB26O38XzwBdTrs9ThOPPzLjfdMTvbF7imPbzGMezwIVAbeLzHyX81tcDC+P2+V+gG9gU5/X53L6Vt5752/23hN/KOuAMQmHgbOAJwnHvWmDnottvoA7oA/0XN3InUFdkuisIP+7D40ZoihvwdbF///glnxinnxY39JsJP/CL4nJekSCA3eLGPC7O65jYPz5xEHqCcKAaFvu/mfdD6zF+YFzcGU4C6glVaZ1sP7CdQDhY7EuoDjwXuDOOezswFxhDSBb7AhPjuEtiLLsBtYSDReMgrM8FwN3ALsB4QiL6ahzXGtftohjLkYSEvk/ie7wwb35P5X0vm+N61xF+pE8SEnw94cf1ZOKzbSQSbWL4LEJCHhX7/wMYyfaD/UN5+1ZvMaVZ3wtifMcBG4GxPWy724GfEPbfA4DVwFviuNOBO3rZ7qfHZX04ft8XEhL6JXG93kbYz0YkYuvp95H7nq8GhsfpVud9Dx1s32fPYnvVY0/f2XExrm8Ad8dxDYQTmE/F+byHcFAd6AQxg3BSVBfX7VHg08n5EJLBzoR9fiThAP65+F2MBA4ptj49rPtVebFcD1wWt+suwL3Af8Zx7yUkjTcQfs97sf1kcNt8E99fsQTRAZwYv+NhcTvfDUyK+8RlwNVFt19/DuLl/AM+CKzMG5Y78+0knFE/QvgR/6qX+bya7WdR8wkZeXZi/PC4YxZKEF8A/jdvfjcRz8YIB6FzE+M+Afwl74fW2wH1Q3k7mBHOOnMJ4s/ARxLjawgHmSnAW4DFhJ2/Jm+aTSTO6hLjyr0+TwDHJfrfTqgaye3UncDwxPhrgS/H7isoniBuTox7FyHR18b+kTG+MYl1OSNvfkfE/eA1PcQ/Js5jdMqYiq3vpuT2iss+tMByJxNKCCMTw74BXBG7T6d4gng80f+6uB7NiWEvAgf08PmLge/nfc+vTYz/NvDzxPeQ3GdrCAfUN/Xwnf0tMe00YFPsfjPhgGiJ8Xfkb+9e1tnTTFfgc58Grk/Oh5iIY/8pwIM9fLbH9elh3ZM1Ec3AFmJJPrGsWxO/w0/1sNxt803sW8USxO154x8F3pron0hIIr2egFdyG8SLwLhk/Zm7v9HdxxCKfp9PTLss+UEzO8TMbjWz1YQz8AmEL+BkQlbdNr2Het0Xe4hhCvBeM3s590c4yExMTLMy0b0RGNHTCsWrHNrj35sI1QrJWDxvXaYAP0gs+yVCEtnN3f9OKC5fAqwys8vNbBShVNJEOHiVdX0K2JVwVpjzdByWs8Z3rEfPH1/M84nuTcAL7t6V6Ice4jWzyYSEdJq7L47Das3sm2b2hJmtI/zIIGzDNIqt74u+Y4N+T9tzV+Ald1+fN6/dUsYBr9w2uHv+sBGw4+/DzNYS2vXy1zm5H+avV3Kf7Sac1PT0PebvT03xN70r8Gzc5wstcwdmdkTefkuy38yO6OFzrzGzG81sZfyOv15kXSdT+LdTbH2KmUIoKa1IrMNlhJJEmuWWKn9bTgGuTyz7UcJJSXNvM6nkBHEXIeOeUGDcVkKROcfNbE8z+4uZzSWcPd5N2Oi/BO4hHFhHE0oek3MfNLOdgFf1EMMywhn3mMTfcHf/Zor4/RUD3Ke7+4j49w/CmVcyFkv2x+X/Z97yh7n7nXF+P3T3GYQzmdcA/02o398M7Fnu9SngOcKOmPPqOCxnrJkN72F8mvn3iZkNA34PXOzuf06Mej9h/zqasG9MzX0kZUzF1jet54CdzWxk3rye7cO80vgNob1jsruPBi5l+zrnJPfD/PVK7rM1hGqLUtd7BbBb3OcLLXMH7n5Hcr+Nw5L78R09fPSnhCrFvd19FKENLH9d85PUHiWuS8GQ8/qXEY5n4xIxj/LtVz4to/BvttC8NgA75XrihTnjUyz/2Lxt1uTuve5jFZsg3P1l4CvAT8zsJDMbaWY1ZnYAoVoo3+XA/4kHzE3ACe6+mXBgOBR4N6Eh+3PAO+MZSQOhjrin7XAV8C4ze3s822wys1Yzm5RiFVYTGpd629n+BEw3s/fEs5D/IpR2ci4FzjGz6QBmNtrM3hu73xDPBOsJO8xmoDue0f0CuMjMdo1xH2ZmjYOwPlcD55rZeDMbB5wXl5n0FTNriCWodxIaJyGcAQ/ED7OQXwCPufu384aPJPxoXyT84PIvcS4WU5r1LcrdlxGqT78Rv5P9CY3TJc8rpZGEEstmMzuYkCjzfdnMdor73ocJDcg5MxL77KcJ2/DuEmO4i3AGe6aZ1ZnZCYQG5YE2ktBQ225mrwU+XmT6G4GJZvZpM2uMx51D+rDc54GpMYHi7isIF318z8xGxWPZnmZ2ZJz+Z8BZZjbDgr3MbEpiXsn9cDGh5PKO+Ps/l9Cu0JtLga/l5hn32UIn3zuo2AQBEH/QnyVUJz0f/y4j1KXPTUxaT2iIvc7MHiI0Wu9tZuuB7xJ23usJjUtfAc4knEWtIJQolvew/GWEM8wvEg6Qywhn6UW3m7tvBL4G/DMW6w4tMM0LhMapbxIOUnsTrkbJjb8e+BYwOxaPHwFy1/ePAv5vjD93RdZ34rizCFf53EeolvoWoZ2irOtDaBy9n9DW8zDhCpcLE+NXxnifA34NfMzdH4vjfg5Mi/P+fbF4SnQy8O5E9V6uiu9XhG33LOGqkfyDXLGYiq1vKU4hlGCeI+yr/+Plu+79E8AF8fdxHqHqLd9thOrZW4DvuvtfE+P+ALyP8F2eCrzH3TtKCcDdtxIapj9CaFf8IOHgvKW0VSnqLEICXE/4vVzT28Sxmu8YQhvXSsLVTUf1Ybm5E58XzeyB2P0hQuP8QsK2+y2xetfdryP8vn4TY/09oeEcQnvUuXE/PMvd1xK+w58R9t0N9HAMS/gBodT41/i93w0UTXy2YxVg9bBw49aN7r6fhbr3Re4+scB0C4CZ8eCImS0lNBSuGsx4s87CHaZXuXua0ooMEStyQ6QN4A1gBeZ9D3Cpu/9yoOctfVPRJYi03H0d8GSi+sXM7PVx9DPAW+PwfQkNuKuHJFAR2cbMjjSzCbGK6TTCJbd/Geq4ZLuqTBBmdjWhDnMfM1tuZh8BPgB8xMzmEW5gydWvfQ74aBx+NXC6V2uxSeRfyz6EmxhfJvxOT4p19VIhqraKSUREyqtsJQgz+4WZrTKzR3oY/wEzm29mD5vZnYkqIRERqQBlK0GY2ZsJd7r+yt33KzD+jcCj7r7GzI4lPA+maKv6uHHjfOrUqX2KacOGDQwfXugK2cpUTfFWU6xQXfFWU6ygeMupP7HOnTv3BXfPv1+id73dZt3fP8Jle4+kmG4s4a7KovOcMWOG99Wtt97a588OhWqKt5pida+ueKspVnfFW079iRW430s8hpe1DSJ5KWqR6c4iPPul4ItyzGwW4SFrNDc3z5g9e3af4mlvb2fEiFKeHDG0qineaooVqiveaooVFG859SfWo446aq67t5T0oVIzSil/pChBEG5CeRR4VZp5qgRRmaopVvfqireaYnVXvOU02CWIIX1BRnykwM8Izwjp6YF5IiIyBIbsPggzezXwO+BUj0/XFBGRylG2EkS8ma2V8Mju5cD/EJ6ZhLtfSngGzKsID+MD6PRS68dERKRsypYg3P2UIuPPILwKT0REKlBVPmpDRETKLzMJYtHK9fzu8a280D7QTxMWEfnXlJkEsWRVOzc80cFLG7YOdSgiIlUhMwki92LDbj2cUEQklcwkiJqYIJQfRETSyUyCiJfSqgQhIpJSdhJE/K/8ICKSTmYSRE0sQShBiIikk5kEoUZqEZHSZCZBbCtBDHEcIiLVIjMJQiUIEZHSZChB5NoglCBERNLITILQfRAiIqXJTIIwcvdBDHEgIiJVIjMJYnsJQhlCRCSNzCQItjVSD20YIiLVIjMJYvtlrsoQIiJpZC9BKD+IiKSSmQSh+yBEREqTmQShy1xFREqTmQQBety3iEgpMpMgtpUghjYMEZGqkZkEoUdtiIiUJjMJQm0QIiKlyVCC0KM2RERKkZkEkaNGahGRdDKTIHSjnIhIaTKTIEwP6xMRKUnZEoSZ/cLMVpnZIz2MNzP7oZktMbP5ZnZQuWIBvXJURKRU5SxBXAHM7GX8scDe8W8W8NMyxrLtKia1QYiIpFO2BOHutwMv9TLJCcCvPLgbGGNmE8sVj+lx3yIiJakbwmXvBixL9C+Pw1bkT2hmswilDJqbm2lrayt5YSvauwFYuGAho9YsLj3aIdDe3t6ndR0K1RQrVFe81RQrKN5yGuxYhzJBpObulwOXA7S0tHhra2vJ81i6uh3uuI3X7rsvrQfuNsARlkdbWxt9WdehUE2xQnXFW02xguItp8GOdSivYnoWmJzonxSHlYVeGCQiUpqhTBA3AB+KVzMdCqx191dULw2UbW0Q3eVagojIv5ayVTGZ2dVAKzDOzJYD/wPUA7j7pcAc4DhgCbAR+HC5YgFd5ioiUqqyJQh3P6XIeAc+Wa7l59Mb5URESpOhO6n1uG8RkVJkJkHocd8iIqXJTIIw9LhvEZFSZCZBbH/lqDKEiEgamUkQ6FEbIiIlyUyCqDE1QoiIlCJzCUIlCBGRdDKTIGL5QfdBiIiklJkEoVeOioiUJjMJAt1JLSJSkswkiNxlriIikk6GEkSukVolCBGRNDKTIPTKURGR0mQmQaiRWkSkNJlJEDmqYhIRSSczCWLbndQiIpJKZhLE9leOqgQhIpJGZhKEXjkqIlKaDCWI8F9tECIi6WQmQZge1iciUpLMJAiIT9tQCUJEJJVMJQhQCUJEJK1MJYga0ytHRUTSylSCAJUgRETSylSCMNNVTCIiaWUqQdSAboQQEUkpUwkClSBERFLLVIKoQVe5ioikVdYEYWYzzWyRmS0xs7MLjH+1md1qZg+a2XwzO66c8YAaqUVE0ipbgjCzWuAS4FhgGnCKmU3Lm+xc4Fp3PxA4GfhJueIJMamKSUQkrXKWIA4Glrj7UnffCswGTsibxoFRsXs08FwZ46FGCUJEJDXzMh0wzewkYKa7nxH7TwUOcfczE9NMBP4KjAWGA0e7+9wC85oFzAJobm6eMXv27D7FdOYt7bxhQj2nTW/s0+cHW3t7OyNGjBjqMFKpplihuuKtplhB8ZZTf2I96qij5rp7SymfqevTkgbOKcAV7v49MzsM+F8z28/du5MTufvlwOUALS0t3tra2qeF1d46hwkTJ9Laun8/wx4cbW1t9HVdB1s1xQrVFW81xQqKt5wGO9ZyVjE9C0xO9E+Kw5I+AlwL4O53AU3AuHIFVAN0qZVaRCSVciaI+4C9zWx3M2sgNELfkDfNM8BbAcxsX0KCWF2ugGoMurqLTyciImVMEO7eCZwJ3AQ8SrhaaYGZXWBmx8fJPgd81MzmAVcDp3u5GkVQI7WISCnK2gbh7nOAOXnDzkt0LwQOL2cMSaEEoQQhIpJGpu6kNiUIEZHUMpUgapUgRERSy1SCqDGjS20QIiKpZCxBQLdKECIiqWQrQYBKECIiKWUqQaiRWkQkvUwlCN0HISKSXuYShEoQIiLpZC5BdOtRGyIiqWQuQaiRWkQknWwlCExVTCIiKWUrQaiRWkQktUwlCF3mKiKSXqYShK5iEhFJL3MJQlVMIiLpZC5BqAQhIpJOthIEoPwgIpJOthKEShAiIqllLEHoPggRkbQyliDUSC0iklZdbyPNrAn4GLAX8DDwc3fvHIzAykH3QYiIpFesBHEl0EJIDscC3yt7RGWkEoSISHq9liCAae7+OgAz+zlwb/lDKh81UouIpFesBNGR66jmqqWcGpQgRETSKlaCeL2ZrQMs9g9L9Lu7jyprdAMsVDENdRQiItWh1wTh7rWDFchgUBWTiEh6xUoQAJjZ64DXxt6F7r6gfCGVT40ZXa5XyomIpFHsMtfRwB+AVwPzCFVLrzOzZ4AT3H1d+UMcOOGVoypBiIikUayR+qvA/cBe7v5udz8R2Bu4D/hasZmb2UwzW2RmS8zs7B6m+XczW2hmC8zsN6WuQClMrxwVEUmtWBXT0cD+7tvrZdy928y+SLg3okdmVgtcAhwDLAfuM7Mb3H1hYpq9gXOAw919jZnt0sf1SKUGcAd3x8yKTi8ikmXFShBbC13eGodtKfLZg4El7r7U3bcCs4ET8qb5KHCJu6+J812VLuy+qYk5oVPVTCIiRRUrQTSZ2YFsv8w1x4DGIp/dDViW6F8OHJI3zWsAzOyfQC1wvrv/JX9GZjYLmAXQ3NxMW1tbkUUX1tWxFTBubbuNhtrKL0G0t7f3eV0HWzXFCtUVbzXFCoq3nAY71mIJYiVwUS/jBmL5ewOtwCTgdjN7nbu/nJzI3S8HLgdoaWnx1tbWPi3sz0/eDGzlsMOPYGRTfT/CHhxtbW30dV0HWzXFCtUVbzXFCoq3nAY71mL3QbT2Y97PApMT/ZPisKTlwD3u3gE8aWaL2d4IPuDqclVMXapiEhEpptc2CDP7fKL7vXnjvl5k3vcBe5vZ7mbWAJwM3JA3ze8JpQfMbByhymlpqsj7oCaubUe37oUQESmmWCP1yYnuc/LGzeztg7Eh+0zgJuBR4Fp3X2BmF5jZ8XGym4AXzWwhcCvw3+7+YuroS5RrdtDd1CIixRVrg7Aeugv1v4K7zwHm5A07L9HtwGfjX9nVqopJRCS1YiUI76G7UH/Fq43XuXZ0qYpJRKSYUp7mmnuSK7G/qayRlUGd7oMQEUktU09zrc01UqsEISJSVLEqpn8paoMQEUkvmwlCl7mKiBSVrQSxrZFaJQgRkWKylSBUxSQiklq2EoTupBYRSS1TCULPYhIRSS9TCSLXBtGpy1xFRIrKVoKIJYgO3SgnIlJUJhOEShAiIsVlKkHUqA1CRCS1TCWIOl3FJCKSWqYSRK2FIoTeByEiUly2EsS2h/UpQYiIFJOtBKFGahGR1LKVIOLa6n0QIiLFZStB5O6DUAlCRKSoTCWIGjNqTJe5ioikkakEAVBXW6PLXEVEUshcgqivMZUgRERSyFyCqKut0VVMIiIpZC5BNNTVsFUJQkSkqMwliMa6GrZ0KEGIiBSTuQTRUFfDFpUgRESKylyCaKyrVQlCRCSFsiYIM5tpZovMbImZnd3LdP9mZm5mLeWMB9QGISKSVtkShJnVApcAxwLTgFPMbFqB6UYCnwLuKVcsSaENomswFiUiUtXKWYI4GFji7kvdfSswGzihwHRfBb4FbC5jLNs0qgQhIpJKORPEbsCyRP/yOGwbMzsImOzufypjHDtorKtha6cShIhIMXVDtWAzqwEuAk5PMe0sYBZAc3MzbW1tfVpme3s7L7+0mTXt3X2ex2Bqb2+vijihumKF6oq3mmIFxVtOgx1rORPEs8DkRP+kOCxnJLAf0GbhTW8TgBvM7Hh3vz85I3e/HLgcoKWlxVtbW/sUUFtbG5MmjmbVMy/T13kMpra2tqqIE6orVqiueKspVlC85TTYsZaziuk+YG8z293MGoCTgRtyI919rbuPc/ep7j4VuBt4RXIYaI11tWzpVCO1iEgxZUsQ7t4JnAncBDwKXOvuC8zsAjM7vlzLLaZBbRAiIqmUtQ3C3ecAc/KGndfDtK3ljCWnsa6GLUoQIiJFZe5OapUgRETSyVyCaKyrpbPb6dJ7qUVEepW5BNFQF1ZZpQgRkd5lLkE0xgShK5lERHqXuQShEoSISDqZSxDbSxBKECIivclcgmhQghARSSVzCaKxrhaAzXrkt4hIrzKXIIY1KEGIiKSRuQQxPCaIjVuVIEREepO5BDFMCUJEJJXMJYidGsLjpzZ1dA5xJCIilS2DCSKUIDZsUQlCRKQ3mUsQuSqmTapiEhHpVeYSxE71aoMQEUkjcwmirraGhroaNqoNQkSkV5lLEBDaITaqDUJEpFfZTBD1tapiEhEpIpMJYlhDrS5zFREpIpMJYnhjnUoQIiJFZDJBDFMVk4hIUZlMEMMb69iwRVVMIiK9yWSCGNlUx/rNShAiIr3JZIIYPayetZs6hjoMEZGKltkEsW5zB93dPtShiIhUrMwmCHdUzSQi0ovMJghA1UwiIr1QghARkYLKmiDMbKaZLTKzJWZ2doHxnzWzhWY238xuMbMp5YwnRwlCRKS4siUIM6sFLgGOBaYBp5jZtLzJHgRa3H1/4LfAt8sVT9LonZQgRESKKWcJ4mBgibsvdfetwGzghOQE7n6ru2+MvXcDk8oYzzZjd2oA4KWNWwdjcSIiVcncy3Opp5mdBMx09zNi/6nAIe5+Zg/T/xhY6e4XFhg3C5gF0NzcPGP27Nl9iqm9vZ0RI0bQ1e2c8deNvGvPet6zd0Of5jUYcvFWg2qKFaor3mqKFRRvOfUn1qOOOmquu7eU8pm6Pi1pgJnZB4EW4MhC4939cuBygJaWFm9tbe3Tctra2sh9dvxdf2PY2F1obd2/T/MaDMl4K101xQrVFW81xQqKt5wGO9ZyJohngcmJ/klx2A7M7GjgS8CR7r6ljPHsYMLoJlau2zxYixMRqTrlbIO4D9jbzHY3swbgZOCG5ARmdiBwGXC8u68qYyyvsMvIJp5XghAR6VHZEoS7dwJnAjcBjwLXuvsCM7vAzI6Pk30HGAFcZ2YPmdkNPcxuwE0Y3agShIhIL8raBuHuc4A5ecPOS3QfXc7l92bS2J14eWMH6zZ3MKqpfqjCEBGpWJm8kxpgz/HhSoAnVrUPcSQiIpUpswlir11CgliiBCEiUlBmE8TkscNoqK1RgpCKsnR1O7cvXj3UYYgAFXIfxFCoq61h311H8eAzLw91KCLbvOV7twHw1DffMcSRiGS4BAFw8NSxPLTsZTZ3dA11KCIiFSfbCWL3V7G1q5u5T68Z6lBERCpOphPEEXuNY3hDLX+c99xQhyIiUnEynSCGNdTytukT+NPDK1i/WY/+FhFJynSCAPjw4VNZv7mTK+98aqhDERGpKJlPEPtPGsMx05r58a1LWLpal7yKiORkPkEAXHjifjTW1XLGr+7nhfZBe6CsiEhFy+x9EEnNo5r42WktnPrzezjhx//kR+8/kINePXaow5IMW/z8+m3dz67v3qG/0ine8lm7pTwveOuJEkT0hqk7M3vWYXziqrm85yd3csIBu3L6G6dywOQxmNlQhyc9WLl2M031NYyJr5Hd3NHFirWb2X3c8G3TPP78evYYP4LamvA9rlq/mXVbt//QtnZ2s2zNxm3P5+qrp17YwITRTTTV1xYc7+4sfr6dfSaMLDqvt33/9h0H/PP2whNWKsVbFsftXr/je5vLTAki4YDJY7jpM2/mJ21PcOWdT/GHh55j93HDOfI14zlk952ZvutoJu88TAmjghz6jVuoMVj6jXDn8WevfYg5D6/ksa/OpKm+lqWr2znm+7fz8dY9+cLM1wJw8NduAeD4t4V5XHDjAq66+xnu/eJb2WVUU5/i6OzqpvW7bRy97y787LQ3FJzmqruf5st/WMA1sw7lkD1eVXCad7xuIn96eAWXvP+gbcMWLFzA9GnT+xTXUFC85fPS048O6vKUIPKMbKrnCzNfyyda9+SGec9x88LnmX3fM1wRr3Ia0VjHpLHDmDR2GLuOGcYuIxsZPayeUcPqGR3/mupraayroTH3v66Gxrpa6mtNyaUMuhOl7nuffAmA9i2dNNXXsm5zJwB3PvFij59/4OnwuJVV67f0OUHkQvjboz2/92pRrMZY/Pz6HhPE8MZaJo5u4h37T9w+7KVFtCb6K53iLZ+2lxYN6vKUIHowsqmeDxwyhQ8cMoXNHV0sfn49C55bx2Mr1vHsy5tYvmYT9zz5EuvjASit2hqjxqDGjBozamsMszC81kICqa0J47du2ULT3X/f4fPJ/JKfawzrcdowPjnOehxXaED++PzPb9ywkZ0euC1/LoPmmIvCsl9o3wrAzIv/wdid6mnfEr6fecte5uiLbtthPXKfeTw+sPFdP76DvfpYzZSsGT7067fQWF9DfW3NDsvLLefLfyXamasAAAuFSURBVFjAd/+6uOB8Nm7tZPyIxj7FIDLQlCBSaKqvZf9JY9h/0phXjNvS2cW6TZ2s3bSVtZs6WLepk80dXWzp7GZLZxdbO7tjdzdbOrroduhyp9ud7m6nq5vQHf+6ukNddVe389yKlUyYsP1M05OHoby2qvymK3fvcbz347OFB8Cq1ZvYZXzxuvWB1tHVzaatXezdHA7sE0Y38Y/HX+Dg3bdfZLDi4ZW0TBnLLqPCgXfNxg7WbNiy7TPNo5q4Y8kLzJw+4RVJtRRLVrVz6B47M2Xn4Wzu7KKjq3uH8XuOH8FfFqzkvTMmMbyx55/ega9+5X4mMhSUIPqpsa6W8SNrGT9y4M/62trW0Nr6+gGfbzm0tbXR2npQ8QkrRIh3xlCHIVLRdB+EiIgUpAQhIiIFKUGIiEhBShAiIlKQEoSIiBSkBCEiIgUpQYiISEFKECIiUpDl3zVb6cxsNfB0Hz8+DnhhAMMpt2qKt5piheqKt5piBcVbTv2JdYq7jy/lA1WXIPrDzO5395ahjiOtaoq3mmKF6oq3mmIFxVtOgx2rqphERKQgJQgRESkoawni8qEOoETVFG81xQrVFW81xQqKt5wGNdZMtUGIiEh6WStBiIhISkoQIiJSUGYShJnNNLNFZrbEzM4exOX+wsxWmdkjiWE7m9nNZvZ4/D82Djcz+2GMcb6ZHZT4zGlx+sfN7LTE8Blm9nD8zA+tny+9NrPJZnarmS00swVm9qlKjdnMmszsXjObF2P9Shy+u5ndE+d/jZk1xOGNsX9JHD81Ma9z4vBFZvb2xPAB3W/MrNbMHjSzG6sg1qfi9/SQmd0fh1XcfpCY3xgz+62ZPWZmj5rZYZUYr5ntE7dp7m+dmX26EmPF3f/l/4Ba4AlgD6ABmAdMG6Rlvxk4CHgkMezbwNmx+2zgW7H7OODPhFdAHwrcE4fvDCyN/8fG7rFx3L1xWoufPbaf8U4EDordI4HFwLRKjDl+fkTsrgfuifO9Fjg5Dr8U+Hjs/gRwaew+Gbgmdk+L+0QjsHvcV2rLsd8AnwV+A9wY+ys51qeAcXnDKm4/SMR2JXBG7G4AxlRyvHGetcBKYEolxlr2A2Ql/AGHATcl+s8BzhnE5U9lxwSxCJgYuycCi2L3ZcAp+dMBpwCXJYZfFodNBB5LDN9hugGK/Q/AMZUeM7AT8ABwCOFO07r87x64CTgsdtfF6Sx/f8hNN9D7DTAJuAV4C3BjXHZFxhrn8RSvTBAVuR8Ao4EniRfeVHq8ifm8DfhnpcaalSqm3YBlif7lcdhQaXb3FbF7JdAcu3uKs7fhywsMHxCxWuNAwpl5RcYcq2weAlYBNxPOol92984C898WUxy/FnhVH9ahry4GPg90x/5XVXCsAA781czmmtmsOKwi9wNCaWo18MtYhfczMxtewfHmnAxcHbsrLtasJIiK5SHFV9y1xmY2Avh/wKfdfV1yXCXF7O5d7n4A4ez8YOC1QxxSQWb2TmCVu88d6lhKcIS7HwQcC3zSzN6cHFlJ+wGhlHUQ8FN3PxDYQKim2abC4iW2Nx0PXJc/rlJizUqCeBaYnOifFIcNlefNbCJA/L8qDu8pzt6GTyowvF/MrJ6QHH7t7r+rhpjd/WXgVkJVyxgzqysw/20xxfGjgRf7sA59cThwvJk9BcwmVDP9oEJjBcDdn43/VwHXExJwpe4Hy4Hl7n5P7P8tIWFUarwQEu8D7v587K+8WPtbh1YNf4Szi6WEYmiuAW/6IC5/Kju2QXyHHRujvh2738GOjVH3xuE7E+pXx8a/J4Gd47j8xqjj+hmrAb8CLs4bXnExA+OBMbF7GPAP4J2EM7Jkw+8nYvcn2bHh99rYPZ0dG36XEhoPy7LfAK1sb6SuyFiB4cDIRPedwMxK3A8SMf8D2Cd2nx9jreR4ZwMfrujfWH939mr5I1wJsJhQR/2lQVzu1cAKoINwlvMRQl3yLcDjwN8SX6oBl8QYHwZaEvP5D2BJ/EvuVC3AI/EzPyavka4P8R5BKNrOBx6Kf8dVYszA/sCDMdZHgPPi8D3iD2QJ4QDcGIc3xf4lcfweiXl9KcaziMQVH+XYb9gxQVRkrDGuefFvQW5+lbgfJOZ3AHB/3B9+TzhoVmS8hKT7IjA6MaziYtWjNkREpKCstEGIiEiJlCBERKQgJQgRESlICUJERApSghARkYKUIKRfzKzZzH5jZkvjIxnuMrN393Oe55vZWbH7AjM7uo/zOcDMjks5bZuZDdmL683sRDOb1sO4j5nZh2L36Wa26wAut9XM3lhoWSJ1xScRKSw+Qvj3wJXu/v44bArh8QH509b59mcOpebu5/UjxAMI14PP6cc8BsuJhAf4Lcwf4e6XJnpPJ1zf/lzaGRfZ9q1AO+FGuPxlScapBCH98RZga/Kg4u5Pu/uPYNvZ7g1m9nfgFjMbYWa3mNkD8Vn1J+Q+Z2ZfMrPFZnYHsE9i+BVmdlLsnmFmt8WSyk2JxxK0mdm3LLwbYrGZvSk+5+YC4H3xmfvvSwZuZsPMbLaF9wZcT7gTOzfubbEk9ICZXRefS4WZfdPCezLmm9l347BmM7vewjsp5uXOxs3sgzGeh8zsMjOrjcPbzexrcdq74+ffSEiq34nT75kX6/lmdlbcDi3Ar+N0w4psk4stvMfhU2b2LgvvlXjQzP4WlzsV+BjwmTi/N+WV3g6IMc6P6zg2Me8dtnccPj2xzvPNbO+S9yipLANxZ6j+svkH/Bfw/V7Gn064ezx3R2gdMCp2jyPc/WnADMIdojsBo+Lws+J0VwAnEd73cCcwPg5/H/CL2N0GfC92Hwf8LbH8H/cQ22cTn98f6CQcfMcBtwPD47gvAOcR7nJdxPb3uOce8XEN4YGGEB55MRrYF/gjUB+H/wT4UOx24F2x+9vAucn17CHW8xPbo414J22KbfKTxDzGJmI/I7G9ts27wLLmA0fG7guIj1/pZXv/CPhA7G4Ahg31Pqq//v2pikkGjJldQnhUx1Z3f0McfLO7v5SbBPi6haeCdhMeQdwMvAm43t03xvncUGD2+wD7ATeHmi1qCY8wyck9VHAu4dlXxbwZ+CGAu883s/lx+KGEl/L8My6nAbiL8LjtzcDPLbwN7sY4/VuAD8X5dAFrzexUQtK7L85jGNsfvLY18dm5hHdt9FWxbXJNonsScE0sYTQQntvTIzMbTUiCt8VBV7LjU0cLbe+7gC+Z2STgd+7+eKkrJJVFCUL6YwHwb7ked/+kmY0jPA8nZ0Oi+wOEB+zNcPcOC082bUq5LAMWuPthPYzfEv930b/92ghJ7ZRXjDA7GHgroURzJiE59DSPK939nALjOjyeYg9QrL1tk+S2/xFwkbvfYGathJJCf7xie7v7b8zsHsLD5eaY2X+6+9/7uRwZQmqDkP74O9BkZh9PDNupl+lHE96J0GFmRxFeswihSufEWKc+EnhXgc8uAsab2WEQHkluZtOLxLee8NrUQm4Hcg3r+xGqmQDuBg43s73iuOFm9prYDjHa3ecAnwFeH6e/Bfh4nLY2nnnfApxkZrvE4TtbaLzva6w9TVfKNhnN9kc+n1Zsue6+FliTa18ATgVuy58uycz2AJa6+w8JbyLcv7fppfIpQUifxTPhE4EjzexJM7uXUBXxhR4+8mugxcweJlTLPBbn8wChOmQe4dHE9xVY1lbCmfu3zGwe4Smzb8yfLs+twLRCjdTAT4ERZvYooX59blzOakLbxdWx2ukuwkuIRgI3xmF3ENowAD4FHBXXaS7hPdALgXMJb2ObT3jT3cQisc4G/js2Iu/Zy3RXAJdaeIteLem3yfnAdWY2l/D60pw/Au/ONVLnfeY0QsP5fMIVYRcUWYd/Bx6Jse1HeGy8VDE9zVVERApSCUJERApSghARkYKUIEREpCAlCBERKUgJQkREClKCEBGRgpQgRESkoP8PQNW4HYifNI0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(edp_list)\n",
    "plt.xlabel(\"Gradient descent iterations\")\n",
    "plt.ylabel(\"EDP\")\n",
    "plt.title(\"Gradient-descent optimization of mapping + architecture\")\n",
    "plt.grid(\"on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(10119563., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.calcMap(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature:  1.0\n",
      "-----------------------\n",
      "tensor(1.4132e+08, grad_fn=<MulBackward0>)\n",
      "tensor(1.4044e+08, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16999984., grad_fn=<MulBackward0>)\n",
      "tensor(11566562., grad_fn=<MulBackward0>)\n",
      "tensor(9905352., grad_fn=<MulBackward0>)\n",
      "tensor(9195953., grad_fn=<MulBackward0>)\n",
      "tensor(8873597., grad_fn=<MulBackward0>)\n",
      "tensor(8706566., grad_fn=<MulBackward0>)\n",
      "tensor(8613530., grad_fn=<MulBackward0>)\n",
      "tensor(8559025., grad_fn=<MulBackward0>)\n",
      "tensor(8526076., grad_fn=<MulBackward0>)\n",
      "tensor(8505949., grad_fn=<MulBackward0>)\n",
      "tensor(8493617., grad_fn=<MulBackward0>)\n",
      "tensor(8486057., grad_fn=<MulBackward0>)\n",
      "tensor(8481417., grad_fn=<MulBackward0>)\n",
      "tensor(8478573., grad_fn=<MulBackward0>)\n",
      "tensor(8476829., grad_fn=<MulBackward0>)\n",
      "tensor(8475757., grad_fn=<MulBackward0>)\n",
      "tensor(8475103., grad_fn=<MulBackward0>)\n",
      "tensor(8474698., grad_fn=<MulBackward0>)\n",
      "tensor(8474454., grad_fn=<MulBackward0>)\n",
      "-----\n",
      "Final EDP:  tensor(8474303., grad_fn=<MulBackward0>)\n",
      "Final map:  tensor([ 5.7557,  1.0000,  1.1221, 19.6578,  1.0000,  2.4532, 18.6267,  1.4193,\n",
      "         1.3717,  1.4193,  1.0000,  1.0000], grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "T = torch.tensor([1.0])\n",
    "print(\"Temperature: \", T.item())\n",
    "print(\"-----------------------\")\n",
    "print(net(T))\n",
    "for i in range(20000):\n",
    "    optimizer.zero_grad() # clear gradients\n",
    "    edp = net(T) # forward step\n",
    "    loss = edp\n",
    "    loss.backward() # backprop\n",
    "    optimizer.step() # optimize weights\n",
    "    if i % 1000 == 0:\n",
    "        print(net(T))\n",
    "print(\"-----\")\n",
    "print(\"Final EDP: \", net(T))\n",
    "print(\"Final map: \", net.calcMap(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final map:  tensor([1.7869, 3.3463, 1.5824, 5.2726, 3.2067, 6.5668, 5.0695, 6.7205, 2.4300,\n",
      "        2.7369, 1.3967, 2.1636], grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Final map: \", net.calcMap(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature:  0.10000000149011612\n",
      "-----------------------\n",
      "tensor(8512693., grad_fn=<MulBackward0>)\n",
      "tensor(8512693., grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8512692., grad_fn=<MulBackward0>)\n",
      "tensor(8512693., grad_fn=<MulBackward0>)\n",
      "tensor(8474068., grad_fn=<MulBackward0>)\n",
      "tensor(8474070., grad_fn=<MulBackward0>)\n",
      "tensor(8474067., grad_fn=<MulBackward0>)\n",
      "tensor(8474067., grad_fn=<MulBackward0>)\n",
      "tensor(8474068., grad_fn=<MulBackward0>)\n",
      "tensor(8474066., grad_fn=<MulBackward0>)\n",
      "tensor(8474067., grad_fn=<MulBackward0>)\n",
      "tensor(8474066., grad_fn=<MulBackward0>)\n",
      "tensor(8474065., grad_fn=<MulBackward0>)\n",
      "tensor(8474066., grad_fn=<MulBackward0>)\n",
      "tensor(8474065., grad_fn=<MulBackward0>)\n",
      "tensor(8474065., grad_fn=<MulBackward0>)\n",
      "tensor(8474068., grad_fn=<MulBackward0>)\n",
      "tensor(8474066., grad_fn=<MulBackward0>)\n",
      "tensor(8474066., grad_fn=<MulBackward0>)\n",
      "tensor(8474065., grad_fn=<MulBackward0>)\n",
      "tensor(8474067., grad_fn=<MulBackward0>)\n",
      "-----\n",
      "Final EDP:  tensor(8474069., grad_fn=<MulBackward0>)\n",
      "Final map:  tensor([ 5.7559,  1.0000,  1.1221, 19.6561,  1.0000,  2.4539, 18.6278,  1.4192,\n",
      "         1.3716,  1.4192,  1.0000,  1.0000], grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "T = torch.tensor([0.1])\n",
    "print(\"Temperature: \", T.item())\n",
    "print(\"-----------------------\")\n",
    "print(net(T))\n",
    "for i in range(20000):\n",
    "    optimizer.zero_grad() # clear gradients\n",
    "    edp = net(T) # forward step\n",
    "    loss = edp\n",
    "    loss.backward() # backprop\n",
    "    optimizer.step() # optimize weights\n",
    "    if i % 1000 == 0:\n",
    "        print(net(T))\n",
    "print(\"-----\")\n",
    "print(\"Final EDP: \", net(T))\n",
    "print(\"Final map: \", net.calcMap(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature:  0.0010000000474974513\n",
      "-----------------------\n",
      "tensor(10119563., grad_fn=<MulBackward0>)\n",
      "tensor(10119563., grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10119563., grad_fn=<MulBackward0>)\n",
      "tensor(10119563., grad_fn=<MulBackward0>)\n",
      "tensor(10119563., grad_fn=<MulBackward0>)\n",
      "tensor(10119563., grad_fn=<MulBackward0>)\n",
      "tensor(10119563., grad_fn=<MulBackward0>)\n",
      "tensor(10119563., grad_fn=<MulBackward0>)\n",
      "tensor(10119563., grad_fn=<MulBackward0>)\n",
      "tensor(10119563., grad_fn=<MulBackward0>)\n",
      "tensor(10119563., grad_fn=<MulBackward0>)\n",
      "-----\n",
      "Final EDP:  tensor(10119563., grad_fn=<MulBackward0>)\n",
      "Final map:  tensor([ 6.,  1.,  1., 25.,  1.,  1., 21.,  1.,  1.,  1.,  1.,  1.],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "T = torch.tensor([0.001])\n",
    "print(\"Temperature: \", T.item())\n",
    "print(\"-----------------------\")\n",
    "print(net(T))\n",
    "for i in range(10000):\n",
    "    optimizer.zero_grad() # clear gradients\n",
    "    edp = net(T) # forward step\n",
    "    loss = edp\n",
    "    loss.backward() # backprop\n",
    "    optimizer.step() # optimize weights\n",
    "    if i % 1000 == 0:\n",
    "        print(net(T))\n",
    "print(\"-----\")\n",
    "print(\"Final EDP: \", net(T))\n",
    "print(\"Final map: \", net.calcMap(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature:  0.009999999776482582\n",
      "-----------------------\n",
      "tensor(9359604., grad_fn=<MulBackward0>)\n",
      "tensor(9359609., grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9359602., grad_fn=<MulBackward0>)\n",
      "tensor(9359610., grad_fn=<MulBackward0>)\n",
      "tensor(9359603., grad_fn=<MulBackward0>)\n",
      "tensor(9359601., grad_fn=<MulBackward0>)\n",
      "tensor(9359624., grad_fn=<MulBackward0>)\n",
      "tensor(9359604., grad_fn=<MulBackward0>)\n",
      "tensor(9359759., grad_fn=<MulBackward0>)\n",
      "tensor(9359612., grad_fn=<MulBackward0>)\n",
      "tensor(9048232., grad_fn=<MulBackward0>)\n",
      "-----\n",
      "Final EDP:  tensor(9048230., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Temperature: \", T.item())\n",
    "print(\"-----------------------\")\n",
    "print(net(T))\n",
    "for i in range(10000):\n",
    "    optimizer.zero_grad() # clear gradients\n",
    "    edp = net(T) # forward step\n",
    "    loss = edp\n",
    "    loss.backward() # backprop\n",
    "    optimizer.step() # optimize weights\n",
    "    if i % 1000 == 0:\n",
    "        print(net(T))\n",
    "print(\"-----\")\n",
    "print(\"Final EDP: \", net(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
